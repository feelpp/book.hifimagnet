[[enduser]]
== *EndUser* role

TIP: when running into some strange portal behavior, first try to `logout/login` from the portal
before reporting an issue.

TIP: make sure that you have access to {uri-sregistry-cesga}[sregistry] holding all the singularity images.
Right now its the only such service (see key `sregistry_url` in blueprints) available in the frame of the project.

[[purchase]]
=== Purchase an application

To purchase an application from the *Marketplace* you can either be *DevUser*
or a simple *EndUser*. The process is the following:

* connect to the {uri-msoportal-ckan-www}[*Marketplace*], in *Home* (top left Marketplace page) you will find all available apps (*All categories*).
* Click on *Add to Cart* to purchase the app you want.
* Click on *Shopping Cart* and *Checkout* to go on
** fill the required forms for *shipping address*
** provide a *shipping address* if not already defined
** provide a *billing address* if not already defined
** Finalize your order by clicking on *Checkout*

To use the app, you will have to wait that the owner/seller of the app validate
you order. To check the status of your order:

* go to *My Inventory*
* go to *Product Orders*

You shall be to see your products and their status in each product form.

To finalize your purchase *DO NOT FORGET* to connect to :

* {uri-sregistry-cesga}[{sregistry}]
* {uri-msoportal-ckan-www}[*Data Catalogue*]
* {uri-girder-unistras}[{girder}]

Then notify the owner/seller that you request to be part of {hifimagnet} user group.
This will allow you to access {himagnet} singularity images and datasets.

NOTE: {girder} is optional.

[[data]]
=== Upload Data to *Data Catalogue*

* Connect to {uri-msoportal-ckan-www}[*Data Catalogue*]
* Select an organization
* Add a dataset if you are assign the role of *editor*

NOTE: you need to ask the owner/seller of {hifimagnet} app to be granted the role of *editor*

[[using_girder]]
=== Upload Data to {girder} service


[[hpc_setting]]
=== Setting up an HPC resource in *Experiments*

* Go to {uri-msoportal-experiment-www}[*Experiments*]
* Select *Settings*
* Define a HPC resource

[source,txt]
----
include::{examplesdir}/hpc.txt[]
----

[[using_sregistry]]
=== Connecting to {sregistry} service

Before proceeding to the setup of the app, you need to make sure, at least for {hifimagnet}
 app (or more generally for apps requiring access to a private singularity image) that you have
 been granted the proper right on {hifimagnet} {sregistry} team.

One more step consists in retrieving the *TOKEN* and storing it into your home directory on the HPC resource.
 This *TOKEN* is needed to download programmatically the {singularity} image :

* connect to {uri-sregistry-cesga}[{sregistry}] using {fiware} authentification
* go to your account (top right button)
* select *Token*

image::sregistry_token.png[ex: {Get {sregistry} token]
image::sregistry_apitoken.png[ex: {Display {sregistry} token]

* copy the token into a file `.sregistry`
* copy `.sregistry` on the HPC resource, see <<hpc_setting, previously defined>>, into your home directory

[NOTE]
====
To copy the file to the HPC resource you can use `scp .sregistry username@host:.`
or whatever method you prefer. `username` and `host` are key defined in the <<hpc_setting,HPC resource setting>>

You may connect to your account on HPC resource
```
ssh -Y username@host
```

If you cannot connect directly to the HPC resource, you may have to use alternative.
On cesga, you can either:

* connect to https://portalusuarios.cesga.es/auth[cesga user portal] with your credentials,
* click on *Create a new Desktop* to get a remote desktop
* in a *terminal* try to `scp user@yourmachine/.sregistry  $HOME`

an alternative consists in:

* a *VPN* solution: use cesga *fortisvpn client*

```
/opt/forticlient-sslvpn/64bit/forticlientsslvpn_cli --server gateway.cesga.es:443 --vpnuser user_email
```

====

[[running]]
=== Running an {hifimagnet} application in *Experiments*

* Select *Create App Instance*
** define an ID
** select a registered app in the menu

At this point you should have a page with all the options that need to be filled before moving to *Deploy Instance*

image::Portal_HifiMagApp.png[ex: Main {hifimagnet} app `3DSim`]

The fields to fill may be grouped into categories:

* specs for running simulations
* HPC settings
* specs retrieving singularity images
* specs for singularity image

The *EndUser* shall only need to fill the fields related to the 1st and 2nd category.
The fields that are to be filled by *EndUser* are marked in *bold*.

The others fields are designed for more advanced users.


[NOTE]
====
.List of keys to be defined for being notified by email (only available in {hifimagnet})
[options="header,footer"]
|===
| Key                        | Description              | Default          | Notes
| *email_user*               | email address            |                  |
| *email_type*               | type of notifications    |                  | workload_manager dependant (for `slurm`: `ALL,END,FAIL`)
|===

.List of keys specs for running simulations
[options="header,footer"]
|===
| Key                           | Description                                 | Default                                                            | Notes
| *Dataset resource: model*     | dataset mode                                | `None`                                                             | comes from the data Catalog
| *Output dataset: outputs_at*  |                                             |                                                                    |
| *execfile*                    | executable file                             | `feelpp_hfm_thermoelectric_model_3D_V1T1_N1`                       | any exec `feelpp_hfm...`
| *cfgfile*                     | configuration file                          | `/usr/share/doc/hifimagnet/ThermoElectricModel/quarter-turn3D.cfg` | cfg file need to be in the container, or in a mounted dir or in the dataset
|===

.List of keys to be defined for HPC settings
[options="header,footer"]
|===
| Key                        | Description              | Default          | Notes
| *monitor_entrypoint*       |  Monitor entrypoint IP   | "193.144.35.146" | for this release it should be empty
| job_prefix                 | Job name prefix in HPCs  | "mso4sc"         |
| *HPC: primary*               | HPC settings             | "cesga"          |
| hpc_modules | modules to load depending on the targeted machine | [gcc/6.3.0, openmpi/2.0.2, singularity/2.4.2] | shall depends on HPC and singularity image
| hpc_volumes | volumes to be mounted on the targeted machine |  [/scratch,/mnt,${LUSTRE}/feel:/feel] | depends on HPC and singularity image
| *hpc_partition* | select partition queue | 'thin-shared' | shall depends on batch system and HPC (TODO)
| hpc_reservation | select reservation queue | `` | is optional
| *parallel_tasks* |  number of tasks/processes to run in parallel | 2 |  shall depends on batch system and selected partition
| *max_time* | maximum allowed time for run (minutes and seconds) | '00:30:00' |  shall depends on batch system and selected partition
|===

.List of keys specs for retrieving singularity images
[options="header,footer"]
|===
| Key                        | Description                                    | Default                        | Notes
| sregistry_client           | define default sregistry client                |  `registry`                    |
| sregistry_client_secrets   | path to file where sregistry secret are stored | `$HOME/.sregistry`             |
| sregistry_storage          | path to container directory                    | `${LUSTRE}/singularity_images` |
| sregistry_url              | URI pointing to the sregistry                  | `sregistry.srv.cesga.es`       |
| sregistry_image            | URI pointing to the sregistry-cli image        | `mso4sc/sregistry`             |
|===

.List of keys specs for singularity image
[options="header,footer"]
|===
| Key                        | Description              | Default          | Notes
| singularity_image_uri      |  URI pointing to the singularity image   | `hifimagnet/hifimagnet:stretch` |
| singularity_image_filename |  Filename of the singularity image       | `hifimagnet-stretch.simg` |
| singularity_image_cleanup  | force remove of singularity image        | `false` |
|===

====

[IMPORTANT]
====
HPC settings for local {lncmi} ressources are specificied xref::lncmi-hpc.adoc[here]
====

Once filled, click on *Create Instance*.
This will result in the executions on the selected HPC resource of the bootstrap script included in the app definition.

TIP: Any error will result in a message at the top of the page, bellow the menus.

The next step is to *Deploy Instance*:

* select the *ID*, you defined in the previous step, in the *App instance * menu
* click on *Deploy*, wait until the button in blue turns green

To *Run Instance*, the procedure is similar:

* select the *ID*, you defined in the previous step, in the *App instance* menu
* click on *RUN*

[TIP]
====
These step may take some minutes to be performed. A red button will indicates an error.
Check the message in the frame bellow the status button. To have more details about the actual error
is more tricky; we recommend to:

* make sure you have unchecked the cleanup option
* connect to the HPC resource
* move to the selected `base_dir/working_prefix<id>`
* check the logs

`base_dir`, `working_prefix` are filled within the portal *Experiments*.
The `<id>` is a random character generated by the deployment.

An other option is to run the app using the *orchestrator CLI*.
see https://github.com/MSO4SC/resources/blob/master/blueprint/feelpp/hifimagnet_test/README.adoc[this file] for details

====

You may cleanup your working by

* UnDeploy your Instance
** select the *ID*, you defined in the previous step, in the *App instance * menu
** click on *UnDeploy*

* Destroy your Instance
** select the *ID*, you defined in the previous step, in the *App instance * menu
** click on *Destroy*

[[running_dataset]]
=== Running an {hifimagnet} application in *Experiments* using datasets from *DataCatalogue*

Basically the procedure is the same as <<running, above>>. The only difference here is that you need:

* to select a dataset in the *DataSet* sliding menu
* to validate your choice 
* to enter the name of the `<cfgfile> you will use

You should at the end have something like in the following image.

image::portal_running_dataset.png: {Selecting a dataset}


[NOTE]
====
The dataset, at least those from {hifimagnet} organization in the *Data Catalogue* are
gzipped archive that holds all the files needed to run:

* a mesh or cad file
* a `<cfgfile>`
* a list of `json` files for physical properties and/or boundary conditions
* eventually some more files when using some {magnettools} features (eg ...)

The format of the files in a dataset are respectively described in:

* for {hifimagnet} simulation 
* for mesh and cad format
* for {magnettoools}

====

[[post]]
=== Setting up an Remote Desktop resource in *Visualization*

* Go to {uri-msoportal-visu-www}[*Visualization*]
* Select *Settings*
* Define a Remote Desktop resource

[source,txt]
----
include::{examplesdir}/remotedesktop.txt[]
----

=== Running an Remote Desktop resource in *Visualization*

* Go to *Visualization*
* Click *Desktop Available*
** Select your *Infrastructure* in the sliding menu

NOTE: if no *Remote desktop* is defined, you will receive a warning message. Create a <<post, *Setting*>>, then go back to this step.

** Click on *Create*
** Click either on *Desktop* or *View Only*

it should start a web page within your browser.
In this page you should be able to:

* start a 'terminal' and check the output of your simulation

The outputs of the job (ie `slurm` jobs) are stored in the `<hpc_basedir>/<hpc_workdir_prefix>_<date>_<id>`.
The actual results for the simulation would be stored in `<hpc_basedir>/feel` directory following {feelpp} usage.

[NOTE]
====
The result are stored in `${LUSTRE}/feel/...` if
you have chosen to use the `<hpc_volumes>` bindmount option: eg `${LUSTRE}/feel:/feel` on cesga.

Check the output of the script (aka `slurm-<id>.out`) in `<basedir>/<workdir_prefix>_<date>_<id>`
to see the exact name of this directory.

In `slurm-<id>.out` you should see lines like that at the beginning of the files:

[source,log]
----
Reading /usr/share/doc/hifimagnet/ThermoElectricModel/quarter-turn3D.cfg...
[ Starting Feel++ ] application thermoelectric version 0.1 date 2018-Jun-07
 . thermoelectric files are stored in /feel/thermoelectric/np_2
 .. exports :/feel/thermoelectric/np_2/exports
 .. logfiles :/feel/thermoelectric/np_2/logs
[modelProperties] Loading Model Properties : "/usr/share/doc/hifimagnet/ThermoElectricModel/quarter-turn3D.json"
 . thermoelectric files are stored in /feel/hifimagnet/ThermoElectricModel/quarter-turn3D.geo/thermoelectric-linear_V1T1_N1/np_2
 .. exports :/feel/hifimagnet/ThermoElectricModel/quarter-turn3D.geo/thermoelectric-linear_V1T1_N1/np_2/exports
 .. logfiles :/feel/hifimagnet/ThermoElectricModel/quarter-turn3D.geo/thermoelectric-linear_V1T1_N1/np_2/logs
----

It means that the actual simulation results are store in `${LUSTRE}/feel/hifimagnet/ThermoElectricModel/quarter-turn3D.geo/thermoelectric-linear_V1T1_N1/np_2/exports/ensightgold/`
====

[NOTE]
====
The `<hpc_basedir>/<hpc_workdir_prefix>_<date>_<id>` directory remains present only if you have set `<do_not_delete_workdir>` to `true`.
The `<id>` field is a random number ...

This directory holds:

* `*.script`
* `slurm-*.out`
* `mso*.err`
* `mso*.out`

and the files coming from the dataset.
====

[TIP]
====
From a computer that is allowed to access HPC resources (if not use a *VPN* client):

* start a terminal,
* connect to the HPC resource with `ssh -Y <username>@<hpc_host>`,
* run `ls -lrt <hpc_basedir>` to find the actual name of the working directory which may appears as the last output of this command.
====

* start a post-processing GUI:
** either by loading {uri-paraview-www}[{paraview}]
** or using singularity {uri-ensight-www}[{ensight}] image
* to quit just click on `Menu/logout` (top left)

For more details on using {paraview} or {ensight} please check XXXX.

[NOTE]
====
In the remote desktop to get {paraview} running:

* open a terminal,
* type: `module load gcc/6.4.0 openmpi/2.1.1 paraview/5.4.0`
====

[NOTE]
====
For {ensight}Â you would need to have:

* a valid license stored in you account on the HPC resource 
* access to {ensight} singularity image from {hifimagnet} {sregistry} private collection.

In the remote desktop to get {ensight} running:

* open a terminal
* use the following script (see {ensight} doc in {hifimagnet} docs).
====

[[offering_setup]]
== To go further

Available apps in **MSO4SC Portal** are:

* MagnetTools: xref::bmap.adoc[AxiMagMap], 
* Preprocessing: xref::MagCAD.adoc[MagCAD],
* Simulations:
** direct models: xref::3Dsim.adoc[MagSim]
** CRB:
// * Postprocessing:

To get more details about the app, click on it.
This will display more information about the app and its settings.

Note that we won't repeat the overwhole process described above to setup, run the app.
We will focuss on inputs that are specific to each app.

